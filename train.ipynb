{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import random\n",
    "import csv\n",
    "\n",
    "from keras import layers\n",
    "\n",
    "seed = 6969\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "replays_dir = pathlib.Path(\"replays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_segment_size = 12\n",
    "post_segment_size = 12\n",
    "prediction_size = 8\n",
    "segment_size = pre_segment_size + post_segment_size + prediction_size\n",
    "\n",
    "note_size = 49\n",
    "\n",
    "input_shape = (segment_size, note_size)\n",
    "\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_leaderboard_id_replays(leaderboard_id):\n",
    "    replay_files = glob.glob(f\"{replays_dir}/{leaderboard_id}/*.npy\")\n",
    "    return leaderboard_id, replay_files\n",
    "\n",
    "\n",
    "def get_leaderboard_replays():\n",
    "    leaderboard_ids = np.array(tf.io.gfile.listdir(str(replays_dir)))\n",
    "    random.shuffle(leaderboard_ids)\n",
    "    val_leaderboard_ids = leaderboard_ids[: int(len(leaderboard_ids) * 0.2)]\n",
    "\n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    for leaderboard_id in leaderboard_ids:\n",
    "        replay_files = glob.glob(f\"{replays_dir}/{leaderboard_id}/*.npy\")\n",
    "\n",
    "        if leaderboard_id in val_leaderboard_ids:\n",
    "            val_data.append((leaderboard_id, replay_files))\n",
    "        else:\n",
    "            train_data.append((leaderboard_id, replay_files))\n",
    "\n",
    "    return train_data, val_data\n",
    "\n",
    "\n",
    "def get_replay_notes(replay, njs):\n",
    "    notes = []\n",
    "\n",
    "    prev_zero_note_time = 0\n",
    "    prev_one_note_time = 0\n",
    "\n",
    "    for note_info, score, note_time in sorted(replay, key=lambda item: item[2]):\n",
    "        type = note_info[-2]\n",
    "\n",
    "        # NOTE: 0-100 score range is rare and often happens for tracking problems that are not important here\n",
    "        # would be good to replace this with acc component only and potentially learn all both acc and swing angles\n",
    "        # but need different format replay files for that\n",
    "        # score = max(0, score - 100)\n",
    "\n",
    "        delta_to_zero = note_time - prev_zero_note_time\n",
    "        delta_to_one = note_time - prev_one_note_time\n",
    "\n",
    "        if type == \"0\":\n",
    "            prev_zero_note_time = note_time\n",
    "            note = preprocess_note(score, delta_to_zero, delta_to_one, note_info, njs)\n",
    "            notes.append(note)\n",
    "        if type == \"1\":\n",
    "            prev_one_note_time = note_time\n",
    "            note = preprocess_note(score, delta_to_one, delta_to_zero, note_info, njs)\n",
    "            notes.append(note)\n",
    "\n",
    "    return notes\n",
    "\n",
    "\n",
    "def preprocess_note(score, delta, delta_other, note_info, njs):\n",
    "    # NOTE: timing increases difficulty not linearly and caps out at ~2 seconds\n",
    "    # no idea if such parameters can be learned by neural networks without adding scaling like I did right here\n",
    "    delta_long = max(0, 2 - delta) / 2\n",
    "    delta_other_long = max(0, 2 - delta_other) / 2\n",
    "    delta_short = max(0, 0.5 - delta) * 2\n",
    "    delta_other_short = max(0, 0.5 - delta_other) * 2\n",
    "\n",
    "    col_number = int(note_info[1])\n",
    "    row_number = int(note_info[2])\n",
    "    direction_number = int(note_info[4])\n",
    "\n",
    "    color = int(note_info[3])\n",
    "\n",
    "    row_col = [0] * 4 * 3\n",
    "    direction = [0] * 10\n",
    "    row_col2 = [0] * 4 * 3\n",
    "    direction2 = [0] * 10\n",
    "\n",
    "    row_col[col_number * 3 + row_number] = 1\n",
    "    direction[direction_number] = 1\n",
    "\n",
    "    response = []\n",
    "\n",
    "    if color == 0:\n",
    "        response.extend(row_col)\n",
    "        response.extend(direction)\n",
    "        response.extend(row_col2)\n",
    "        response.extend(direction2)\n",
    "        response.extend(\n",
    "            [\n",
    "                delta_short,\n",
    "                delta_long,\n",
    "            ]\n",
    "        )\n",
    "        response.extend(\n",
    "            [\n",
    "                delta_other_short,\n",
    "                delta_other_long,\n",
    "            ]\n",
    "        )\n",
    "    if color == 1:\n",
    "        response.extend(row_col2)\n",
    "        response.extend(direction2)\n",
    "        response.extend(row_col)\n",
    "        response.extend(direction)\n",
    "        response.extend(\n",
    "            [\n",
    "                delta_other_short,\n",
    "                delta_other_long,\n",
    "            ]\n",
    "        )\n",
    "        response.extend(\n",
    "            [\n",
    "                delta_short,\n",
    "                delta_long,\n",
    "            ]\n",
    "        )\n",
    "    response.extend([njs / 30, score])\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def create_segments(notes):\n",
    "    empty_res = ([], [])\n",
    "    if len(notes) < prediction_size:\n",
    "        return empty_res\n",
    "\n",
    "    segments = []\n",
    "    predictions = []\n",
    "    for i in range(len(notes) - prediction_size + 1):\n",
    "        if i % prediction_size != 0:\n",
    "            continue\n",
    "\n",
    "        pre_slice = notes[max(0, i - pre_segment_size) : i]\n",
    "        slice = notes[i : i + prediction_size]\n",
    "        post_slice = notes[\n",
    "            i + prediction_size : i + prediction_size + post_segment_size\n",
    "        ]\n",
    "\n",
    "        # NOTE: using relative score can be good to find relative difficulty of the notes more fairly\n",
    "        # because good players will always get higher acc and worse players will do badly even on easy patterns\n",
    "\n",
    "        pre_segment = [np.array(note[:-1]) for note in pre_slice]\n",
    "        if len(pre_segment) < pre_segment_size:\n",
    "            pre_segment[0:0] = [\n",
    "                np.zeros(note_size, dtype=np.float32)\n",
    "                for i in range(pre_segment_size - len(pre_segment))\n",
    "            ]\n",
    "\n",
    "        segment = [np.array(note[:-1]) for note in slice]\n",
    "\n",
    "        post_segment = [np.array(note[:-1]) for note in post_slice]\n",
    "        if len(post_segment) < post_segment_size:\n",
    "            post_segment.extend(\n",
    "                [\n",
    "                    np.zeros(note_size, dtype=np.float32)\n",
    "                    for i in range(post_segment_size - len(post_segment))\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # fix this pls\n",
    "        prediction = [note[-1] for note in slice]\n",
    "\n",
    "        final_segment = []\n",
    "        final_segment.extend(pre_segment)\n",
    "        final_segment.extend(segment)\n",
    "        final_segment.extend(post_segment)\n",
    "        segments.append(final_segment)\n",
    "\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    return segments, predictions\n",
    "\n",
    "\n",
    "def to_id(value):\n",
    "    if int(value) < 30000:\n",
    "        return str(30000 + int(value))\n",
    "    return str(int(value))\n",
    "\n",
    "\n",
    "def preprocess_leaderboard_replays(leaderboard_replays, print_progress=False):\n",
    "    asd = {}\n",
    "    replays = []\n",
    "\n",
    "    for leaderboard_replay in leaderboard_replays:\n",
    "        arr = np.load(leaderboard_replay)\n",
    "        sort_indices = np.argsort(arr[:, 2])\n",
    "\n",
    "        # Sort the entire array column-wise based on these indices\n",
    "        sorted_arr = arr[sort_indices]\n",
    "        replays.append(sorted_arr)\n",
    "\n",
    "    for replay in replays:\n",
    "        if len(asd.values()) == 0:\n",
    "            for values in replay:\n",
    "                asd[to_id(values[0]) + str(values[2] * 100)] = [\n",
    "                    to_id(values[0]),\n",
    "                    [values[1]],\n",
    "                    values[2],\n",
    "                    1,\n",
    "                ]\n",
    "        else:\n",
    "            for values in replay:\n",
    "                key = to_id(values[0]) + str(values[2] * 100)\n",
    "                if key in asd:\n",
    "                    asd[key][1].append(values[1])\n",
    "                    asd[key][2] += values[2]\n",
    "                    asd[key][3] += 1\n",
    "\n",
    "    asd2 = []\n",
    "    for values in asd.values():\n",
    "        if values[3] < 8 or len(values[0]) > 5:\n",
    "            return ([], [])\n",
    "\n",
    "        acc_list = values[1]\n",
    "        acc_list.sort()\n",
    "        acc = sum(acc_list) / len(acc_list) if len(acc_list) > 0 else 0\n",
    "        asd2.append([values[0], acc, values[2] / values[3]])\n",
    "\n",
    "    notes = get_replay_notes(\n",
    "        asd2, float(leaderboard_replays[0].split(\"-\")[3].replace(\".npy\", \"\"))\n",
    "    )\n",
    "    return create_segments(notes)\n",
    "\n",
    "\n",
    "def generate_data(leaderboards_replays):\n",
    "    segments = []\n",
    "    scores = []\n",
    "\n",
    "    for (leaderboard_id, leaderboard_replays) in leaderboards_replays:\n",
    "        print(f\"Processing leaderboard {leaderboard_id}\")\n",
    "        segment, score = preprocess_leaderboard_replays(leaderboard_replays)\n",
    "        if len(segment) == 0:\n",
    "            continue\n",
    "\n",
    "        segments.extend(segment)\n",
    "        scores.extend(score)\n",
    "\n",
    "    return np.array(segments), np.array(scores)\n",
    "\n",
    "\n",
    "def clamp(num, lower, higher):\n",
    "    return min([max([num, lower]), higher])\n",
    "\n",
    "\n",
    "train_data, val_data = get_leaderboard_replays()\n",
    "test_data = val_data\n",
    "\n",
    "train_x, train_y = generate_data(train_data)\n",
    "val_x, val_y = generate_data(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import Bidirectional, LSTM, Concatenate, Cropping1D, TimeDistributed, Dense\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=input_shape, name=\"input_1\")\n",
    "\n",
    "bidirectional_1 = layers.Bidirectional(layers.LSTM(units=12, return_sequences=True, activation=\"tanh\"), backward_layer=layers.LSTM(units=12, return_sequences=True, go_backwards=True, activation=\"tanh\"))(input_layer)\n",
    "bidirectional_2 = layers.Bidirectional(layers.LSTM(units=10, return_sequences=True, activation=\"tanh\"), backward_layer=layers.LSTM(units=10, return_sequences=True, go_backwards=True, activation=\"tanh\"))(bidirectional_1)\n",
    "\n",
    "# Concatenate the output of the second bidirectional LSTM layer with the input\n",
    "concatenate = Concatenate()([bidirectional_2, input_layer])\n",
    "\n",
    "# Cropping1D layer\n",
    "cropping1d = Cropping1D(cropping=(12, 12))(concatenate)  # Adjust the cropping as necessary\n",
    "\n",
    "# TimeDistributed layer with Dense layer inside\n",
    "time_distributed_1 = TimeDistributed(Dense(units=16, activation='relu'))(cropping1d)\n",
    "\n",
    "# Another TimeDistributed layer with Dense layer inside\n",
    "time_distributed_2 = TimeDistributed(Dense(units=16, activation='relu'))(time_distributed_1)\n",
    "\n",
    "# Final TimeDistributed layer with Dense layer for output\n",
    "time_distributed_3 = TimeDistributed(Dense(units=1, activation='linear'), name=\"time_distributed_2\")(time_distributed_2)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=input_layer, outputs=time_distributed_3)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "    loss=tf.keras.losses.MeanAbsoluteError(reduction=\"sum_over_batch_size\"),\n",
    "    metrics=['mae', 'mse'],\n",
    ")\n",
    "\n",
    "log_dir = \"logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir, histogram_freq = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "global initial_epoch\n",
    "initial_epoch = 0\n",
    "\n",
    "class EpochCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        global initial_epoch\n",
    "        initial_epoch += 1\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return tf.abs(y_true - y_pred)**1\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "# cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     monitor='val_mae',\n",
    "#     filepath=f\"models/{'speed' if speed_stuff else 'acc'}/{start_time.strftime('%Y-%m-%d-%H-%M-%S')}\",\n",
    "#     save_best_only=True,\n",
    "#     save_weights_only=False,\n",
    "#     verbose=1)\n",
    "\n",
    "\n",
    "# model2 = tf.keras.models.load_model(\"model_sleep_lstm_acc\", custom_objects={\"custom_loss\": custom_loss})\n",
    "# model = tf.keras.models.load_model(f'model_sleep_2gru_{\"speed\" if speed_stuff else \"acc\"}')\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=False),\n",
    "    # optimizer=tf.keras.optimizers.SGD(learning_rate=0.05, momentum=0.9, nesterov=True),\n",
    "    # loss=tf.keras.losses.Huber(delta=5),\n",
    "    loss='mse',\n",
    "    metrics=['mae', 'mse'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    validation_data=(val_x, val_y),\n",
    "    callbacks=[tensorboard_callback, EpochCallback()],\n",
    "    batch_size=128,\n",
    "    epochs=initial_epoch + 20,\n",
    "    initial_epoch=initial_epoch,\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tf2onnx\n",
    "import onnx\n",
    "\n",
    "model.save(\"model_sleep_bl.keras\")\n",
    "\n",
    "# Convert the saved model to ONNX\n",
    "spec = (tf.TensorSpec((None, segment_size, note_size), tf.float32, name=\"input_1\"),)\n",
    "output_path = \"model_sleep_bl.onnx\"\n",
    "model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=None, output_path=output_path)\n",
    "\n",
    "print(f\"Model is converted to ONNX and saved at {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "hmm = [\n",
    "\t[\"31fbd91\", \"Sequence Breaker\"],\n",
    "\t[\"d90a11\", \"watch me dance es\"]\n",
    "]\n",
    "for val_data_this, map_name in hmm:\n",
    "\tval_xx, val_yy = generate_data([get_leaderboard_id_replays(val_data_this)])\n",
    "\tif len(val_xx) == 0:\n",
    "\t\tcontinue\n",
    "\n",
    "\tacc = []\n",
    "\tacc_real = []\n",
    "\n",
    "\tspeeds = [[], [], []]\n",
    "\tspeeds_real = [[], [], []]\n",
    "\n",
    "\tcomplexities = []\n",
    "\tcomplexities_real = []\n",
    "\n",
    "\tfor batch_pred, batch_inp in zip(model(val_xx), val_yy):\n",
    "\t\tfor pred, inp in zip(batch_pred, batch_inp):\n",
    "\t\t\tif inp == 0.0:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tacc.append(pred[0])\n",
    "\t\t\tacc_real.append(inp)\n",
    "\n",
    "\t\t\t# for i in range(3):\n",
    "\t\t\t#     speeds[i].append(pred[i+1])\n",
    "\t\t\t#     speeds_real[i].append(inp[i+1])\n",
    "\n",
    "\t\t\t# complexities.append(pred[1]*(1-pred[0]))\n",
    "\t\t\t# complexities_real.append(inp[1]*(1-inp[0]))\n",
    "\n",
    "\n",
    "\tprint(f\"{val_data_this} {map_name} predicted acc - {(sum(acc)/len(acc)*15+100)/1.15} -- real acc {(sum(acc_real)/len(acc_real)*15+100)/1.15}\")\n",
    "\t# for i in range(3):\n",
    "\t#     print(f\"{val_data_this} speed - {sum(speeds[i])/len(speeds[i])} -- {sum(speeds_real[i])/len(speeds_real[i])}\")\n",
    "\t# print(f\"{val_data_this} complexity - {sum(complexities)/len(complexities)} -- {sum(complexities_real)/len(complexities_real)}\")\n",
    "\n",
    "\tprint(\"\")\n",
    "\tprint(\"\")\n",
    "\n",
    "\t# complexities = np.convolve(complexities, np.ones(30)/30, mode='valid')\n",
    "\t# speed = np.convolve(speed, np.ones(30)/30, mode='valid')\n",
    "\t# acc = np.convolve([(acc_*15+100)/115 for acc_ in acc], np.ones(30)/30, mode='valid')\n",
    "\tacc_real = np.convolve(acc_real, np.ones(15)/15, mode='valid')\n",
    "\tx = [i for i in range(len(acc_real))]\n",
    "\tplt.plot(x, acc_real)\n",
    "\tacc = np.convolve(acc, np.ones(15)/15, mode='valid')\n",
    "\tx = [i for i in range(len(acc))]\n",
    "\tplt.plot(x, acc)\n",
    "\n",
    "\t# plt.plot(x, complexities)\n",
    "\t# for i in range(3):\n",
    "\t#     plt.plot(x, np.convolve(speeds[i], np.ones(30)/30, mode='valid'))\n",
    "\tplt.xlabel('x - axis')\n",
    "\tplt.ylabel('y - axis')\n",
    "\tplt.tick_params(axis='x', colors='white')\n",
    "\tplt.tick_params(axis='y', colors='white')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for leaderboard_id, leaderboard_replays in test_data:\n",
    "    try:\n",
    "        curr, score = preprocess_leaderboard_replays(leaderboard_replays)\n",
    "\n",
    "        _predictions = model.predict(np.array(curr))\n",
    "\n",
    "        real_sum = 0\n",
    "        for prediction in score:\n",
    "            real_sum += prediction[0]\n",
    "\n",
    "        real_avg = real_sum / len(score)\n",
    "        real_percentage_score = (100 + real_avg * 15) / 115\n",
    "\n",
    "        prediction_sum = 0\n",
    "        for prediction in _predictions:\n",
    "            prediction_sum += prediction[0]\n",
    "\n",
    "        avg = prediction_sum / len(_predictions)\n",
    "        percentage_score = (100 + avg[0] * 15) / 115\n",
    "\n",
    "        predictions.append(\n",
    "            [\n",
    "                f\"https://beatleader.xyz/leaderboard/global/{leaderboard_id}\",\n",
    "                round(percentage_score, 5),\n",
    "                round(real_percentage_score, 5),\n",
    "                abs(round(real_percentage_score - percentage_score, 5)),\n",
    "            ]\n",
    "        )\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "with open(\"predictions.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    header = [\"LeaderboardId\", \"Prediction\", \"Expected\", \"Difference\"]\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for prediction in predictions:\n",
    "        writer.writerow(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
